name: CI - Lint, Test, and Validate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install flake8 pylint black bandit
        
    - name: Run Black (Code Formatting Check)
      run: |
        black --check --diff generate_scripts.py lib/
      continue-on-error: true
      
    - name: Run Flake8 (Style Guide)
      run: |
        flake8 generate_scripts.py lib/ --max-line-length=120 --extend-ignore=E203,W503 \
          --format='::error file=%(path)s,line=%(row)d,col=%(col)d::%(code)s: %(text)s'
      continue-on-error: true
      
    - name: Run Pylint (Static Analysis)
      run: |
        pylint generate_scripts.py lib/*.py --max-line-length=120 \
          --disable=C0103,C0114,C0115,C0116,R0913,R0914,R0915,W0718 \
          --output-format=text || true
      continue-on-error: true
      
    - name: Run Bandit (Security Scan)
      run: |
        bandit -r generate_scripts.py lib/ -f json -o bandit-report.json || true
        bandit -r generate_scripts.py lib/ -f txt || true
      continue-on-error: true
      
    - name: Upload Security Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-report
        path: bandit-report.json
        retention-days: 30

  test:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html pytest-json-report
        
    - name: Run Unit Tests
      run: |
        pytest tests/ -v \
          --cov=lib \
          --cov=generate_scripts \
          --cov-report=term-missing \
          --cov-report=html:htmlcov \
          --cov-report=xml:coverage.xml \
          --html=test-report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=test-results.json || echo "Tests not yet implemented"
      continue-on-error: true
      
    - name: Generate Test Summary
      if: always()
      run: |
        echo "## Test Results Summary 🧪" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f test-results.json ]; then
          python -c "
        import json
        import sys
        try:
            with open('test-results.json') as f:
                data = json.load(f)
            summary = data.get('summary', {})
            print(f\"| Metric | Count |\")
            print(f\"|--------|-------|\")
            print(f\"| ✅ Passed | {summary.get('passed', 0)} |\")
            print(f\"| ❌ Failed | {summary.get('failed', 0)} |\")
            print(f\"| ⚠️ Skipped | {summary.get('skipped', 0)} |\")
            print(f\"| 🕐 Duration | {summary.get('duration', 0):.2f}s |\")
        except:
            print('No test results available yet')
        " >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ No test results found - tests not yet implemented" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results
        path: |
          test-report.html
          test-results.json
          htmlcov/
        retention-days: 30
        
    - name: Upload Coverage Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-report
        path: coverage.xml
        retention-days: 30
        
  validation:
    name: Configuration Validation
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Validate Example Configurations
      run: |
        echo "## Configuration Validation Results 📋" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Config File | Status | Issues |" >> $GITHUB_STEP_SUMMARY
        echo "|-------------|--------|--------|" >> $GITHUB_STEP_SUMMARY
        
        for config in examples/configs/*.json; do
          if [ -f "$config" ]; then
            config_name=$(basename "$config")
            echo "Validating $config_name..."
            if python3 generate_scripts.py --config "$config" --validate-only 2>&1 | tee validation.log; then
              echo "| $config_name | ✅ Valid | None |" >> $GITHUB_STEP_SUMMARY
            else
              issues=$(grep -i "error\|warning" validation.log | wc -l || echo "0")
              echo "| $config_name | ❌ Invalid | $issues |" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done
        
    - name: Test Script Generation (Dry Run)
      run: |
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Script Generation Test 🔧" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        config_file="examples/configs/config_nonpartitioned_to_interval_hash.json"
        if [ -f "$config_file" ]; then
          if python3 generate_scripts.py --config "$config_file" 2>&1 | tee generation.log; then
            echo "✅ Script generation successful" >> $GITHUB_STEP_SUMMARY
            
            # Count generated files
            if [ -d "output" ]; then
              file_count=$(find output -type f -name "*.sql" | wc -l)
              echo "📄 Generated $file_count SQL files" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "❌ Script generation failed" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        
    - name: Upload Generated Scripts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: generated-scripts-sample
        path: output/
        retention-days: 7

  summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [lint, test, validation]
    if: always()
    
    steps:
    - name: Generate Overall Summary
      run: |
        echo "# 🎯 CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality & Linting | ${{ needs.lint.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests & Coverage | ${{ needs.test.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Configuration Validation | ${{ needs.validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
